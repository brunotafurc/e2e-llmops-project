{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be63921b-4820-4872-a708-bf5722e0f8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow databricks-openai databricks-agents threadpoolctl==3.1.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af65798-0acc-4263-bc35-9212653d3987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"btafur_catalog\")\n",
    "dbutils.widgets.text(\"schema_name\", \"default\")\n",
    "dbutils.widgets.text(\"model_name\", \"quickstart_agent\")\n",
    "dbutils.widgets.text(\"logged_run_id\", \"None\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "logged_run_id = dbutils.widgets.get(\"logged_run_id\")\n",
    "\n",
    "dbutils.widgets.text(\"experiment_name\", f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/{model_name}_{catalog_name}\")\n",
    "experiment_name = dbutils.widgets.get(\"experiment_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec9759b-b50d-4611-be17-d3b51a3041a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "registered_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "model_uri = f\"runs:/{logged_run_id}/agent\"\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a33257-a9da-4f65-8c8a-657612d6d634",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756760635746}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM {catalog_name}.{schema_name}.labelled_sentences\"\n",
    "df = spark.sql(query).toPandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c997521-a041-46ed-8b37-b0b9ddf67df3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import Guidelines, Correctness, RelevanceToQuery\n",
    "import datetime\n",
    "\n",
    "eval_data = []\n",
    "for _, row in df.iterrows():\n",
    "    eval_item = {\n",
    "        \"inputs\": {\n",
    "            \"content\": row[\"review\"]  # This matches the function parameter name\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": str(row[\"sentiment\"])  # Adjust column name\n",
    "        }\n",
    "    }\n",
    "    eval_data.append(eval_item)\n",
    "\n",
    "mlflow_eval_dataset = None\n",
    "    \n",
    "try:\n",
    "    # Try to get existing dataset\n",
    "    mlflow_eval_dataset = mlflow.genai.get_dataset(f\"{catalog_name}.{schema_name}.mlflow_eval_dataset\")\n",
    "    print(f\"✓ Using existing dataset: {catalog_name}.{schema_name}.mlflow_eval_dataset\")\n",
    "    \n",
    "except Exception as get_error:\n",
    "    print(f\"Dataset not found, creating dataset\")\n",
    "    \n",
    "    try:\n",
    "        # Create new dataset\n",
    "        mlflow_eval_dataset = mlflow.genai.create_dataset(f\"{catalog_name}.{schema_name}.mlflow_eval_dataset\")\n",
    "        mlflow_eval_dataset.merge_records(eval_data)\n",
    "        dataset_created = True\n",
    "        print(f\"✓ Created new dataset: {catalog_name}.{schema_name}.mlflow_eval_dataset\")\n",
    "        \n",
    "    except Exception as create_error:\n",
    "        print(f\"⚠ Could not create dataset {create_error}\")\n",
    "\n",
    "if (mlflow_eval_dataset is not None):       \n",
    "        \n",
    "    guidelines = {\n",
    "        \"sentiment_accuracy\": \"Response must correctly identify sentiment\",\n",
    "        \"clarity\": [\"Response must be clear and concise\"]\n",
    "    }\n",
    "\n",
    "    agent = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "    def predict_function(content):\n",
    "        try:\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content\n",
    "                }\n",
    "            ]\n",
    "            prediction = agent.predict({\"messages\": messages})\n",
    "            return prediction\n",
    "        except Exception as e:\n",
    "            print(f\"Error in predict_fn: {e}\")\n",
    "            return {\"response\": \"PREDICTION_ERROR\"}\n",
    "        \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with mlflow.start_run(run_id=logged_run_id) as run:\n",
    "        with mlflow.start_run(run_name=f\"sentiment_agent_evaluation_{timestamp}\", nested=True) as eval_run:\n",
    "            results = mlflow.genai.evaluate(\n",
    "                data=mlflow_eval_dataset,\n",
    "                predict_fn=predict_function,\n",
    "                scorers=[\n",
    "                    Correctness(),\n",
    "                    Guidelines(name=\"sentiment_accuracy\", guidelines=guidelines[\"sentiment_accuracy\"]),\n",
    "                    Guidelines(name=\"clarity\", guidelines=guidelines[\"clarity\"]),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    # Print results\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "281e07cf-0163-453a-9256-c508b37cd073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "registered_model = mlflow.register_model(model_uri, name=registered_model_name)\n",
    "\n",
    "if results.metrics['sentiment_accuracy/mean'] > 0.9:\n",
    "  print(\"Transitioning to champion\")\n",
    "  client.set_registered_model_alias(registered_model_name, \"Champion\",registered_model.version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "model_evaluation",
   "widgets": {
    "catalog_name": {
     "currentValue": "btafur_catalog",
     "nuid": "3a964089-b210-4dc6-aa79-c7c287530d31",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "btafur_catalog",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "btafur_catalog",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "experiment_name": {
     "currentValue": "/Users/bruno.tafur@databricks.com/quickstart_agent_btafur_catalog",
     "nuid": "d243520b-4f07-4843-b294-0e3b3489d85b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Users/bruno.tafur@databricks.com/quickstart_agent_btafur_catalog",
      "label": null,
      "name": "experiment_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Users/bruno.tafur@databricks.com/quickstart_agent_btafur_catalog",
      "label": null,
      "name": "experiment_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "logged_run_id": {
     "currentValue": "None",
     "nuid": "970893b9-068c-4fff-af42-e9b346618f94",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "None",
      "label": null,
      "name": "logged_run_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "None",
      "label": null,
      "name": "logged_run_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "model_name": {
     "currentValue": "quickstart_agent",
     "nuid": "06926edb-a96f-4f02-a702-f7c542e5ef3c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "quickstart_agent",
      "label": null,
      "name": "model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "quickstart_agent",
      "label": null,
      "name": "model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "febc8306-c467-4694-9bfe-6467bb3558a2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
